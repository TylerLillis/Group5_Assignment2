<html>
    <head>
        <title>The IT World - Give Me Five</title>
        <!-- Importing google fonts -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,900;1,400;1,900&display=swap" rel="stylesheet"> 
        <!-- Importing external stylesheet and icon -->
        <link rel="stylesheet" href="stylesheet.css">
        <link rel="icon" type="image/x-icon" href="resources/favicon.png">
    </head>
    <body>

        <!-- Top bar of website -->
        <div style="overflow: auto; margin: 25px; margin-top: 20px;">
            <a href="index.html#ittechs"><b><h4 style="float: left" class="TopLinks">Back</h4></b></a>
        </div>
        
        <!-- IT Tech Essay -->
        <div class="ContainerDiv">
            <h3>Machine Learning</h3>
            <div class="MainPanel">
                <p>
                    Machine learning is the use of a combination of computer algorithms and data sets to ‘teach’ a computer to make decisions or predictions without being programmed by a human. This field of study has evolved and flourished since the term’s coining in 1959[1], especially in today’s world and it focuses on automation and smart devices. Its accessibility has become a major advancement in the field, as the greater demand for stronger and faster computers has allowed the public to start partaking in the field. One such example of this is ‘Folding@home,’ a project that allows people with computers to donate computing power to help a model simulate the folding of proteins, which aids the study of Alzheimer's, and was later adapted to help study the novel coronavirus[2]. This is done through the use of ‘distributed computing[3], a method that allows for large scale datasets to be broken up and processed on 2 or more different computers. This method allows a cheaper alternative to using an expensive supercomputer for complex models, while also not sacrificing processing power. All of this is used in conjunction with the ever-expanding human genome dataset[4] that is constantly being sequenced and leads to complex simulations of how proteins in the human body fold and react to foreign stimuli such as viruses.<br><br>

                    Due to continued hardware advancements in recent years, the computational power we have access to now allows for even more complex algorithms and faster result yielding. This has allowed for machine learning to start being used in extremely complex fields of quantum and theoretical sciences, allowing for simulations at the atomic level to achieve accuracy far beyond what humans could ever hope to achieve. In a joint development by the University of Belas and the University of Bristol in the field of computational chemistry, scientists created a new representation for the simulation and prediction of theoretical compounds. The results from this development were found to be more accurate readings, while also reducing the machine learning time for a QM9 (Quantum Machines 9) dataset, which is the benchmark for a model and contains over 133, 000 molecules, as it was reduced from three weeks of training, down to one day[5].<br><br>

                    As for the future of machine learning technology, further advancements in overall computing power will of course improve the speed and accuracy at which a model will work, but the true innovations lie in machines known as Learnware[6]. These are pre-trained machines with specifications that help differentiate between the purposes and specialties of models. Advancements in these areas will improve a model’s ability to train more efficiently with large datasets, as well as allow the model to continue functioning effectively even if the environment changes<br><br>

                    These developments have societal altering potential for change. It has the possibility of affecting how entire industries make data-driven decisions, as machine learning becomes more complex and makes increasingly accurate predictions and decisions. Areas such as Business, advertising and banking have embraced the concept, along with Governments, whose need to handle national sized datasets containing citizens' personal information and variables have been aided by advancements in the field. But Machine learning's introduction to various other fields has yet to fully take hold. This is exemplified by the medical system, as healthcare presents one the largest and most impactful opportunities to improve an industry using machine learning. As a whole, data in the industry is input and analyzed manually, and this can lead to issues when dealing with increasingly more information as we find new ways to measure things. One of the ways this is done is by streamlining and eventually phasing out some professions, such as Radiologist and anatomical pathologists which rely upon manually analyzing data in the form of digitalized images, as the model learns to ‘transform data into knowledge[7]’, in a capacity that far exceeds human capabilities. Machine learning also holds the potential to improve diagnostic accuracy, though its capacity to phase out diagnosticians is further into the future due to the relative lack of clarity compared to the field of pathology and radiology. And as was mentioned previously, its potential to help studies into various illnesses presents a major change in medical science.<br><br>

                    While machine learning macro influences are ginormous, it also has become a major player on the micro and individual level. The use of algorithms has become synonymous with personal internet usage. One of the most common interactions a person might have with machine learning, even if they do not realize it, is personalized advertisements on the internet. Advertisers utilize algorithms that use cookies to personalize ads for users based on their web history data. Another way increased machine learning capabilities are impacting the lives of everyday people is through innovations in self-driving cars. With further development of this technology, traffic, as a concept could all be irradicated in the future, with the machine, learnt cars interacting with one and synchronizing to flow as smoothly as a possible. Another potential advantage of autonomous cars is reducing road casualties, and finally ‘start the year-on-year decreases[8]’ Of road fatalities. Digital assistants are another personalized use of machine learning, with their capabilities of learning what the user’s needs and likes are and starting to anticipate decisions. One of the drawbacks of machine learning is that it requires rich amounts of data to make more informed predictions, there is a concern for user privacy and data security. These concerns have led to studies in this field[9] and may become less of an issue in the future. <br><br>
                </p>
                <h2 class="ITWorkHeading">References</h2>
                <p>
                    [1] Samuel, Arthur (1959). "Some Studies in Machine Learning Using the Game of Checkers". IBM Journal of Research and Development. 3 (3): 210–229. CiteSeerX 10.1.1.368.2254. doi:10.1147/rd.33.0210.<br><br>
                    [2] Forti, A., Glushkov, I., Heinrich, L., Lassnig, M., South, D., & Walker, R. (2021). The fight against COVID-19: Running Folding@Home simulations on ATLAS resources. Les Ulis: EDP Sciences. doi:http://dx.doi.org/10.1051/epjconf/202125102003 <br><br>
                    [3] Rajeev Pandey, Sanjay Silakari, Investigations on optimizing performance of the distributed computing in heterogeneous environment using machine learning technique for large scale data set, Materials Today: Proceedings, 2021, ISSN 2214-7853, https://doi.org/10.1016/j.matpr.2021.07.089., (https://www.sciencedirect.com/science/article/pii/S2214785321049415) <br><br>
                    [4] Jie Huang, Xinming Liang, Yuankai Xuan, Chunyu Geng, Yuxiang Li, Haorong Lu, Shoufang Qu, Xianglin Mei, Hongbo Chen, Ting Yu, Nan Sun, Junhua Rao, Jiahao Wang, Wenwei Zhang, Ying Chen, Sha Liao, Hui Jiang, Xin Liu, Zhaopeng Yang, Feng Mu, Shangxian Gao, A reference human genome dataset of the BGISEQ-500 sequencer, GigaScience, Volume 6, Issue 5, May 2017, gix024, https://doi.org/10.1093/gigascience/gix024 <br><br>
                    [5] Anders S. Christensen, Lars A. Bratholm, Felix A. Faber, and O. Anatole von Lilienfeld , "FCHL revisited: Faster and more accurate quantum machine learning", J. Chem. Phys. 152, 044107 (2020) https://doi-org.ezproxy.lib.rmit.edu.au/10.1063/1.5126701 <br><br>
                    [6] Zhou, Zhi-Hua. “Learnware： on the Future of Machine Learning.” Frontiers of Computer Science 10.4 (2016): 589–590. Web.<br><br>
                    [7] Obermeyer, Ziad, and Ezekiel J Emanuel. “Predicting the Future — Big Data, Machine Learning, and Clinical Medicine.” The New England journal of medicine 375.13 (2016): 1216–1219. Web. <br><br>
                    [8] Soni, Abhishek et al. “Design of a Machine Learning-Based Self-Driving Car.” Machine Learning for Robotics Applications. Singapore: Springer Singapore, 2021. 139–151. Web. <br><br>
                    [9] Zheng, Huadi, Haibo Hu, and Ziyang Han. “Preserving User Privacy for Machine Learning: Local Differential Privacy or Federated Machine Learning?” IEEE intelligent systems 35.4 (2020): 5–14. Web. <br>
                </p>
            </div>
        </div>

    </body>
</html>